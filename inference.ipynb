{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tacotron import Tacotron\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "import librosa\n",
    "from synthesizer import Synthesizer\n",
    "from speech_encoder import SpeechEncoder\n",
    "from speech_encoder_v2 import SpeechEncoderV2\n",
    "from data_preprocessing import *\n",
    "import torchaudio\n",
    "from embed import Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wav, sample_rate = torchaudio.load(\"D:\\CODING\\SpeechEncoder\\data\\LibriSpeech/train-clean-100/2764/36616/2764-36616-0000.flac\")\n",
    "wav = preprocess_audio(wav, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_device = torch.device(\"cpu\")\n",
    "\n",
    "# encoder = SpeechEncoder(device, loss_device)\n",
    "encoder = SpeechEncoderV2(device, device)\n",
    "\n",
    "# checkpoints = torch.load(\"models\\speech_encoder_lstm\\encoder.pt\")\n",
    "checkpoints = torch.load(\"models\\speech_encoder_transformer\\encoder(0.096).pt\")\n",
    "\n",
    "encoder.load_state_dict(checkpoints['model_state'])\n",
    "embedder = Embed(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, partial_embeds, _ = embedder.embed_utterance(wav, return_partials=True)\n",
    "# embedding = np.expand_dims(embedding, 0)\n",
    "text = \"Last weekend, I went to the zoo with my family. We saw lions, elephants, and monkeys. The birds were colorful and sang beautiful songs. It was exciting to see so many animals in one place.\".split(\"\\n\")\n",
    "embeddings = [embedding] * len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_model_path = Path(\"models/synthesizer/synthesizer.pt\")\n",
    "synthesizer = Synthesizer(synthesizer_model_path, embeddings)\n",
    "\n",
    "synthesizer.load()\n",
    "specs = synthesizer.synthesize_spectrograms(text)\n",
    "spec = np.concatenate(specs, axis=1)\n",
    "\n",
    "breaks = [spec.shape[1] for spec in specs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WAV using the Griffiin Lim Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_og = synthesizer.griffin_lim(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ends = np.cumsum(np.array(breaks) * Synthesizer.hparams.hop_size)\n",
    "b_starts = np.concatenate(([0], b_ends[:-1]))\n",
    "wavs = [wav_og[start:end] for start, end, in zip(b_starts, b_ends)]\n",
    "breaks = [np.zeros(int(0.15 * Synthesizer.sample_rate))] * len(breaks)\n",
    "wav = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_og = preprocess_audio(wav_og, Synthesizer.sample_rate)\n",
    "wav_processed = wav_og / np.abs(wav_og).max() * 0.97\n",
    "\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(wav, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WAV using the Vocoder (credits: Corentin Jemine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocoder import Vocoder\n",
    "\n",
    "vocoder = Vocoder()\n",
    "vocoder.load_model(\"models/vocoder/vocoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_new = vocoder.infer_waveform(spec)\n",
    "\n",
    "# b_ends = np.cumsum(np.array(breaks) * Synthesizer.hparams.hop_size)\n",
    "# b_starts = np.concatenate(([0], b_ends[:-1]))\n",
    "# wavs = [wav_new[start:end] for start, end, in zip(b_starts, b_ends)]\n",
    "# breaks = [np.zeros(int(0.15 * Synthesizer.sample_rate))] * len(breaks)\n",
    "# wav_vocoder = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])\n",
    "\n",
    "# wav_vocoder = wav_new / np.abs(wav_new).max() * 0.97    \n",
    "\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(wav_new, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
