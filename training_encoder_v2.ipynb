{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech_encoder_v2 import SpeechEncoderV2\n",
    "from params import *\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import utils\n",
    "import visualisations\n",
    "from data_processor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a20891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync(device: torch.device):\n",
    "    # For correct profiling (cuda operations are async)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68a6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"run_id\": \"speech_encoder_transformer\",  # A unique identifier for this training run\n",
    "    \"clean_data_root\": \"D:/CODING/SpeechEncoder/data/processed_audio\",  # Path to LibriSpeech dataset\n",
    "    \"models_dir\": \"models\",  # Directory to save model checkpoints\n",
    "    \"umap_every\": 500,  # Update UMAP visualization every 500 steps\n",
    "    \"save_every\": 500,  # Save model checkpoint every 500 steps\n",
    "    \"backup_every\": 5000,  # Create a backup copy of the model every 5000 steps\n",
    "    \"vis_every\": 100,  # Update visualization metrics every 100 steps\n",
    "    \"force_restart\": False,  # Whether to restart training from scratch\n",
    "    \"visdom_server\": \"http://localhost\",  # Visdom server address for visualization\n",
    "    \"no_visdom\": False,  # Whether to disable Visdom visualization\n",
    "    \"models_dir\": Path(\"models\"),  # Directory to save model checkpoints\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8adb3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeakerVerificationDataset(Path(\"D:/CODING/SpeechEncoder/data/his_processed_audio\"))\n",
    "loader = SpeakerVerificationDataLoader(\n",
    "        dataset,\n",
    "        40,\n",
    "        10,\n",
    "        num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef00c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 160, 40)\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch.data.shape)\n",
    "    break  # Check the shape of the first batch only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86063299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5de626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the model and the optimizer\n",
    "model = SpeechEncoderV2(device, device)\n",
    "model.to(device)\n",
    "checkpoints = torch.load(\"models\\speech_encoder_transformer\\encoder(0.096).pt\")\n",
    "model.load_state_dict(checkpoints['model_state'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_init)\n",
    "init_step = 1\n",
    "\n",
    "# Configure file path for the model\n",
    "model_dir = params['models_dir'] / params['run_id']\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "state_fpath = model_dir / \"encoder.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add209aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 13/250000000 [01:03<313932:37:57,  4.52s/step, eer=0.0125, loss=0.128]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = params['models_dir'] / \"logs\"\n",
    "\n",
    "# Initialize TensorBoard writer (ensure log_dir is defined)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Initialize the progress bar\n",
    "total_steps = len(loader)  # Assuming `loader` has a defined length\n",
    "progress_bar = tqdm(enumerate(loader, init_step), total=total_steps, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step, speaker_batch in progress_bar:\n",
    "    # Forward pass\n",
    "    inputs = torch.from_numpy(speaker_batch.data).to(device)\n",
    "    sync(device)\n",
    "    embeds = model(inputs)\n",
    "    sync(device)\n",
    "    embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)\n",
    "    loss, eer = model.loss(embeds_loss)\n",
    "    sync(loss_device)\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    model.do_gradient_ops()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log scalars to TensorBoard\n",
    "    writer.add_scalar(\"Loss\", loss.item(), step)\n",
    "    writer.add_scalar(\"EER\", eer, step)\n",
    "    \n",
    "    # Update the progress bar with the current loss and EER\n",
    "    progress_bar.set_postfix({\"loss\": loss.item(), \"eer\": eer})\n",
    "\n",
    "    # Save the model every 'save_every' steps with a unique filename that includes the step and loss\n",
    "    if params['save_every'] != 0 and step % params['save_every'] == 0:\n",
    "        filename = model_dir / f\"encoder_{step:06d}_loss_{loss.item():.4f}.pt\"\n",
    "        print(\"Saving the model (step %d) to %s\" % (step, filename))\n",
    "        torch.save({\n",
    "            \"step\": step + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    # Make a backup every 'backup_every' steps\n",
    "    if params['backup_every'] != 0 and step % params['backup_every'] == 0:\n",
    "        print(\"Making a backup (step %d)\" % step)\n",
    "        backup_fpath = model_dir / f\"encoder_{step:06d}.bak\"\n",
    "        torch.save({\n",
    "            \"step\": step + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, backup_fpath)\n",
    "    \n",
    "    # Update the progress bar with loss and EER information.\n",
    "    progress_bar.set_postfix(loss=loss.item(), eer=eer)\n",
    "    \n",
    "# Optionally, close the writer after training\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
