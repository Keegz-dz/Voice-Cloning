{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech_encoder_v2 import SpeechEncoderV2\n",
    "from params import *\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import utils\n",
    "import visualisations\n",
    "from data_processor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a20891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync(device: torch.device):\n",
    "    # For correct profiling (cuda operations are async)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68a6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"run_id\": \"speech_encoder_transformer\",  # A unique identifier for this training run\n",
    "    \"clean_data_root\": \"D:/CODING/SpeechEncoder/data/processed_audio\",  # Path to LibriSpeech dataset\n",
    "    \"models_dir\": \"models\",  # Directory to save model checkpoints\n",
    "    \"umap_every\": 500,  # Update UMAP visualization every 500 steps\n",
    "    \"save_every\": 500,  # Save model checkpoint every 500 steps\n",
    "    \"backup_every\": 5000,  # Create a backup copy of the model every 5000 steps\n",
    "    \"vis_every\": 100,  # Update visualization metrics every 100 steps\n",
    "    \"force_restart\": False,  # Whether to restart training from scratch\n",
    "    \"visdom_server\": \"http://localhost\",  # Visdom server address for visualization\n",
    "    \"no_visdom\": False,  # Whether to disable Visdom visualization\n",
    "    \"models_dir\": Path(\"models\"),  # Directory to save model checkpoints\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8adb3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeakerVerificationDataset(Path(\"D:/CODING/SpeechEncoder/data/his_processed_audio\"))\n",
    "loader = SpeakerVerificationDataLoader(\n",
    "        dataset,\n",
    "        40,\n",
    "        10,\n",
    "        num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef00c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 160, 40)\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    print(batch.data.shape)\n",
    "    break  # Check the shape of the first batch only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86063299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5de626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the model and the optimizer\n",
    "model = SpeechEncoderV2(device, device)\n",
    "model.to(device)\n",
    "checkpoints = torch.load(\"models\\speech_encoder_transformer\\encoder(0.096).pt\")\n",
    "model.load_state_dict(checkpoints['model_state'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_init)\n",
    "init_step = 1\n",
    "\n",
    "# Configure file path for the model\n",
    "model_dir = params['models_dir'] / params['run_id']\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "state_fpath = model_dir / \"encoder.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add209aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 20/250000000 [01:32<322111:21:05,  4.64s/step, eer=0.0172, loss=0.108]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader, init_step), total\u001b[38;5;241m=\u001b[39mtotal_steps, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, speaker_batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(speaker_batch\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m     sync(device)\n",
      "File \u001b[1;32mc:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\speaker_verification_dataset.py:61\u001b[0m, in \u001b[0;36mSpeakerVerificationDataLoader.collate\u001b[1;34m(self, speakers)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate\u001b[39m(\u001b[38;5;28mself\u001b[39m, speakers):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSpeakerBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeakers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutterances_per_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartials_n_frames\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\speaker_batch.py:16\u001b[0m, in \u001b[0;36mSpeakerBatch.__init__\u001b[1;34m(self, speakers, utterances_per_speaker, n_frames)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, speakers: List[Speaker], utterances_per_speaker: \u001b[38;5;28mint\u001b[39m, n_frames: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeakers \u001b[38;5;241m=\u001b[39m speakers\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartials \u001b[38;5;241m=\u001b[39m {s: s\u001b[38;5;241m.\u001b[39mrandom_partial(utterances_per_speaker, n_frames) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m speakers}\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Array of shape (n_speakers * n_utterances, n_frames, mel_n), e.g. for 3 speakers with\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# 4 utterances each of 160 frames of 40 mel coefficients: (12, 160, 40)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([frames \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m speakers \u001b[38;5;28;01mfor\u001b[39;00m _, frames, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartials[s]])\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\speaker_batch.py:16\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, speakers: List[Speaker], utterances_per_speaker: \u001b[38;5;28mint\u001b[39m, n_frames: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeakers \u001b[38;5;241m=\u001b[39m speakers\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartials \u001b[38;5;241m=\u001b[39m {s: \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mutterances_per_speaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m speakers}\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Array of shape (n_speakers * n_utterances, n_frames, mel_n), e.g. for 3 speakers with\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# 4 utterances each of 160 frames of 40 mel coefficients: (12, 160, 40)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([frames \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m speakers \u001b[38;5;28;01mfor\u001b[39;00m _, frames, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartials[s]])\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\speaker.py:44\u001b[0m, in \u001b[0;36mSpeaker.random_partial\u001b[1;34m(self, count, n_frames)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_utterances()\n\u001b[0;32m     42\u001b[0m utterances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutterance_cycler\u001b[38;5;241m.\u001b[39msample(count)\n\u001b[1;32m---> 44\u001b[0m a \u001b[38;5;241m=\u001b[39m [(u,) \u001b[38;5;241m+\u001b[39m u\u001b[38;5;241m.\u001b[39mrandom_partial(n_frames) \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m utterances]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\speaker.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_utterances()\n\u001b[0;32m     42\u001b[0m utterances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutterance_cycler\u001b[38;5;241m.\u001b[39msample(count)\n\u001b[1;32m---> 44\u001b[0m a \u001b[38;5;241m=\u001b[39m [(u,) \u001b[38;5;241m+\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_frames\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m utterances]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\utterance.py:26\u001b[0m, in \u001b[0;36mUtterance.random_partial\u001b[1;34m(self, n_frames)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrandom_partial\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_frames):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    Crops the frames into a partial utterance of n_frames\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    partial utterance in the complete utterance.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m n_frames:\n\u001b[0;32m     28\u001b[0m         start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\CODING\\Voice-Cloning\\data_processor\\utterance.py:16\u001b[0m, in \u001b[0;36mUtterance.get_frames\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_frames\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes_fpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anike\\anaconda3\\envs\\gpu_environment\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = params['models_dir'] / \"logs\"\n",
    "\n",
    "# Initialize TensorBoard writer (ensure log_dir is defined)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Initialize the progress bar\n",
    "total_steps = len(loader)  # Assuming `loader` has a defined length\n",
    "progress_bar = tqdm(enumerate(loader, init_step), total=total_steps, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step, speaker_batch in progress_bar:\n",
    "    # Forward pass\n",
    "    inputs = torch.from_numpy(speaker_batch.data).to(device)\n",
    "    sync(device)\n",
    "    embeds = model(inputs)\n",
    "    sync(device)\n",
    "    embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)\n",
    "    loss, eer = model.loss(embeds_loss)\n",
    "    sync(loss_device)\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    model.do_gradient_ops()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log scalars to TensorBoard\n",
    "    writer.add_scalar(\"Loss\", loss.item(), step)\n",
    "    writer.add_scalar(\"EER\", eer, step)\n",
    "    \n",
    "    # Update the progress bar with the current loss and EER\n",
    "    progress_bar.set_postfix({\"loss\": loss.item(), \"eer\": eer})\n",
    "\n",
    "    # Save the model every 'save_every' steps with a unique filename that includes the step and loss\n",
    "    if params['save_every'] != 0 and step % params['save_every'] == 0:\n",
    "        filename = model_dir / f\"encoder_{step:06d}_loss_{loss.item():.4f}.pt\"\n",
    "        print(\"Saving the model (step %d) to %s\" % (step, filename))\n",
    "        torch.save({\n",
    "            \"step\": step + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    # Make a backup every 'backup_every' steps\n",
    "    if params['backup_every'] != 0 and step % params['backup_every'] == 0:\n",
    "        print(\"Making a backup (step %d)\" % step)\n",
    "        backup_fpath = model_dir / f\"encoder_{step:06d}.bak\"\n",
    "        torch.save({\n",
    "            \"step\": step + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, backup_fpath)\n",
    "    \n",
    "    # Update the progress bar with loss and EER information.\n",
    "    progress_bar.set_postfix(loss=loss.item(), eer=eer)\n",
    "    \n",
    "# Optionally, close the writer after training\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
