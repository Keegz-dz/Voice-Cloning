{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the required modules and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from utils.tacotron import Tacotron\n",
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "import librosa\n",
    "from synthesizer import Synthesizer\n",
    "from speech_encoder import SpeechEncoder\n",
    "from speech_encoder_v2 import SpeechEncoderV2\n",
    "from data_preprocessing import *\n",
    "import torchaudio\n",
    "from embed import Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wav, sample_rate = torchaudio.load(\"D:\\CODING\\SpeechEncoder\\data\\LibriSpeech/train-clean-100/2764/36616/2764-36616-0000.flac\")\n",
    "wav = preprocess_audio(wav, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the speaker encoder model and loading the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_device = torch.device(\"cpu\")\n",
    "\n",
    "# encoder = SpeechEncoder(device, loss_device)\n",
    "encoder = SpeechEncoderV2(device, device)\n",
    "\n",
    "# checkpoints = torch.load(\"models\\speech_encoder_lstm\\encoder.pt\")\n",
    "checkpoints = torch.load(\"..\\models\\speech_encoder_transformer\\encoder(0.096).pt\")\n",
    "\n",
    "encoder.load_state_dict(checkpoints['model_state'])\n",
    "embedder = Embed(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, partial_embeds, _ = embedder.embed_utterance(wav, return_partials=True)\n",
    "# embedding = np.expand_dims(embedding, 0)\n",
    "text = \"Last weekend, I went to the zoo with my family. We saw lions, elephants, and monkeys. The birds were colorful and sang beautiful songs. It was exciting to see so many animals in one place.\".split(\"\\n\")\n",
    "embeddings = [embedding] * len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer_model_path = Path(\"..\\models/synthesizer/synthesizer.pt\")\n",
    "synthesizer = Synthesizer(synthesizer_model_path)\n",
    "\n",
    "synthesizer.load()\n",
    "specs = synthesizer.synthesize_spectrograms(text, embeddings)\n",
    "spec = np.concatenate(specs, axis=1)\n",
    "\n",
    "breaks = [spec.shape[1] for spec in specs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WAV using the Griffiin Lim Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_og = synthesizer.griffin_lim(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ends = np.cumsum(np.array(breaks) * Synthesizer.params.hop_size)\n",
    "b_starts = np.concatenate(([0], b_ends[:-1]))\n",
    "wavs = [wav_og[start:end] for start, end, in zip(b_starts, b_ends)]\n",
    "breaks = [np.zeros(int(0.15 * Synthesizer.sample_rate))] * len(breaks)\n",
    "wav = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_og = preprocess_audio(wav_og, Synthesizer.sample_rate)\n",
    "wav_processed = wav_og / np.abs(wav_og).max() * 0.97\n",
    "\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(wav, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating WAV using the Vocoder (Corentin Jemine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocoder import Vocoder\n",
    "\n",
    "vocoder = Vocoder()\n",
    "vocoder.load_model(\"..\\models/vocoder/vocoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_new = vocoder.infer_waveform(spec)\n",
    "\n",
    "b_ends = np.cumsum(np.array(breaks) * Synthesizer.params.hop_size)\n",
    "b_starts = np.concatenate(([0], b_ends[:-1]))\n",
    "wavs = [wav_new[start:end] for start, end, in zip(b_starts, b_ends)]\n",
    "breaks = [np.zeros(int(0.15 * Synthesizer.sample_rate))] * len(breaks)\n",
    "wav_vocoder = np.concatenate([i for w, b in zip(wavs, breaks) for i in (w, b)])\n",
    "\n",
    "wav_vocoder = wav_new / np.abs(wav_new).max() * 0.97    \n",
    "\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(wav_new, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
