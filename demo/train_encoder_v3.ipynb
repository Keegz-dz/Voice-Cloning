{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from speech_encoder_v5 import SpeechEncoderV5\n",
    "from params import *\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import utils\n",
    "import visualisations\n",
    "from data_scripts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a20891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync(device: torch.device):\n",
    "    # For correct profiling (cuda operations are async)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"run_id\": \"speech_encoder_transformer_v5\",  # A unique identifier for this training run\n",
    "    \"clean_data_root\": \"D:/CODING/SpeechEncoder/data/his_processed_audio\",  # Path to LibriSpeech dataset\n",
    "    \"models_dir\": \"models\",  # Directory to save model checkpoints\n",
    "    \"umap_every\": 500,  # Update UMAP visualization every 500 steps\n",
    "    \"save_every\": 500,  # Save model checkpoint every 500 steps\n",
    "    \"backup_every\": 5000,  # Create a backup copy of the model every 5000 steps\n",
    "    \"vis_every\": 100,  # Update visualization metrics every 100 steps\n",
    "    \"force_restart\": False,  # Whether to restart training from scratch\n",
    "    \"visdom_server\": \"http://localhost\",  # Visdom server address for visualization\n",
    "    \"no_visdom\": False,  # Whether to disable Visdom visualization\n",
    "    \"models_dir\": Path(\"../models\"),  # Directory to save model checkpoints\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, loader = load_data(params['clean_data_root'], 40, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    print(batch.data.shape)\n",
    "    break  # Check the shape of the first batch only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86063299",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5de626",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the model and the optimizer\n",
    "model = SpeechEncoderV5(device, device, num_speakers=40)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# checkpoints = torch.load(\"..\\models\\speech_encoder_transformer_v5\\encoder_001500_loss_4.3992.pt\")\n",
    "# model.load_state_dict(checkpoints['model_state'])\n",
    "\n",
    "# Calculate total training steps (adjust based on your dataset size)\n",
    "total_training_steps = 250_000_000  # Match your target step count\n",
    "\n",
    "# AdamW optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,  # Start with lower base LR\n",
    "    weight_decay=0.01  # Reduce from 0.05\n",
    ")\n",
    "\n",
    "# OneCycle learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,      # Peak learning rate\n",
    "    total_steps=total_training_steps,\n",
    "    pct_start=0.1,    # Warmup percentage\n",
    "    anneal_strategy='linear'\n",
    ")\n",
    "\n",
    "init_step = 1\n",
    "\n",
    "# Configure file path for the model\n",
    "model_dir = params['models_dir'] / params['run_id']\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "state_fpath = model_dir / \"encoder.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add209aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = params['models_dir'] / \"logs\"\n",
    "\n",
    "# Initialize TensorBoard writer (ensure log_dir is defined)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Initialize the progress bar\n",
    "total_steps = len(loader)  # Assuming `loader` has a defined length\n",
    "progress_bar = tqdm(enumerate(loader, init_step), total=total_steps, desc=\"Training\", unit=\"step\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step, speaker_batch in progress_bar:\n",
    "    # Forward pass\n",
    "    inputs = torch.from_numpy(speaker_batch.data).to(device)\n",
    "    sync(device)\n",
    "    embeds = model(inputs)\n",
    "    sync(device)\n",
    "    embeds_loss = embeds.view((speakers_per_batch, utterances_per_speaker, -1)).to(loss_device)\n",
    "    loss, eer = model.loss(embeds_loss)\n",
    "    sync(loss_device)\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    model.do_gradient_ops()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log scalars to TensorBoard\n",
    "    writer.add_scalar(\"Loss\", loss.item(), step)\n",
    "    writer.add_scalar(\"EER\", eer, step)\n",
    "    \n",
    "    # Update the progress bar with the current loss and EER\n",
    "    progress_bar.set_postfix({\"loss\": loss.item(), \"eer\": eer})\n",
    "\n",
    "    # Save the model every 'save_every' steps with a unique filename that includes the step and loss\n",
    "    if params['save_every'] != 0 and step % params['save_every'] == 0:\n",
    "        filename = model_dir / f\"encoder_{step:06d}_loss_{loss.item():.4f}.pt\"\n",
    "        print(\"Saving the model (step %d) to %s\" % (step, filename))\n",
    "        torch.save({\n",
    "            \"step\": step + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    # Make a backup every 'backup_every' steps\n",
    "    if params['backup_every'] != 0 and step % params['backup_every'] == 0:\n",
    "        print(\"Making a backup (step %d)\" % step)\n",
    "        backup_fpath = model_dir / f\"encoder_{step:06d}.bak\"\n",
    "        torch.save({\n",
    "            \"step\": step + 1,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        }, backup_fpath)\n",
    "    \n",
    "    # Update the progress bar with loss and EER information.\n",
    "    progress_bar.set_postfix(loss=loss.item(), eer=eer)\n",
    "    \n",
    "# Optionally, close the writer after training\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
